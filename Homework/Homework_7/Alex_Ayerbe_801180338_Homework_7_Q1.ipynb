{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Build a Convolutional Neural Network, like what we built in lectures to classify the images across all 10 classes in CIFAR 10. You need to adjust the fully connected layer at the end properly concerning the number of output classes. Train your network for 200 epochs"
      ],
      "metadata": {
        "id": "WPevEledriCR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rV4AiCEYO1M-",
        "outputId": "8c556509-abe2-47fb-965d-235ac94957fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data-unversions/p1ch7/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:13<00:00, 12.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data-unversions/p1ch7/cifar-10-python.tar.gz to ../data-unversions/p1ch7/\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import time\n",
        "\n",
        "# Define the data path and Load CIFAR-10 dataset\n",
        "datapath = '../data-unversions/p1ch7/'\n",
        "cifar10 = datasets.CIFAR10(root= datapath, train=True, download = True, transform=transforms.ToTensor())\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-processing. Seperating training and validation datasets."
      ],
      "metadata": {
        "id": "Srm3M2kHO49I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute dataset mean and standard deviation for normalization\n",
        "imgs = torch.stack([img_t for img_t, _ in cifar10], dim=3)\n",
        "mean = imgs.view(3, -1).mean(dim=1)\n",
        "std = imgs.view(3, -1).std(dim=1)\n",
        "\n",
        "# Define normalization transformation\n",
        "normalize_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=dataset_mean, std=dataset_std)\n",
        "])\n",
        "\n",
        "train_data = datasets.CIFAR10(root=data_path, train=True, download=True, transform=normalize_transform)\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "\n",
        "val_data = datasets.CIFAR10(root=data_path, train=False, download=True, transform=normalize_transform)\n",
        "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtAXtKIEPIdb",
        "outputId": "efc246f3-41bb-4b38-9b1b-7b1c264e4228"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolution Neural Network Training and Validation"
      ],
      "metadata": {
        "id": "iVKhOQ6cPKsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFAR10Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CIFAR10Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.tanh(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = CIFAR10Net().to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 200\n",
        "total_start_time = time.time()\n",
        "for epoch in range(epochs):\n",
        "    start_time = time.time()\n",
        "    model.train()\n",
        "    losses = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses += loss.item()\n",
        "\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {losses/len(train_loader)}, Training Time: {training_time:.2f} seconds')\n",
        "\n",
        "total_end_time = time.time()\n",
        "total_training_time = total_end_time - total_start_time\n",
        "print(f'Total Training Time: {total_training_time:.2f} seconds')\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in val_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\\\n",
        "\n",
        "val_accuracy = correct / total\n",
        "print(f'Test Accuracy: {val_accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Wdw8hGmPJ1_",
        "outputId": "dd83ce4a-6973-4521-ff63-f42bf779e8f9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200, Loss: 2.1598667445582453, Training Time: 16.75 seconds\n",
            "Epoch 11/200, Loss: 1.2879153241427832, Training Time: 16.33 seconds\n",
            "Epoch 21/200, Loss: 1.0669298431313503, Training Time: 16.87 seconds\n",
            "Epoch 31/200, Loss: 0.8988889748479644, Training Time: 16.53 seconds\n",
            "Epoch 41/200, Loss: 0.752425080945838, Training Time: 16.14 seconds\n",
            "Epoch 51/200, Loss: 0.6081888225413407, Training Time: 16.72 seconds\n",
            "Epoch 61/200, Loss: 0.464308940593966, Training Time: 16.64 seconds\n",
            "Epoch 71/200, Loss: 0.32798325690373503, Training Time: 16.12 seconds\n",
            "Epoch 81/200, Loss: 0.2137395437527984, Training Time: 16.57 seconds\n",
            "Epoch 91/200, Loss: 0.1338190478764317, Training Time: 16.57 seconds\n",
            "Epoch 101/200, Loss: 0.08530234694433228, Training Time: 16.50 seconds\n",
            "Epoch 111/200, Loss: 0.05731386684062423, Training Time: 16.18 seconds\n",
            "Epoch 121/200, Loss: 0.04106898987767068, Training Time: 15.93 seconds\n",
            "Epoch 131/200, Loss: 0.031129694405540358, Training Time: 15.96 seconds\n",
            "Epoch 141/200, Loss: 0.02469899345747054, Training Time: 15.89 seconds\n",
            "Epoch 151/200, Loss: 0.02020505720288305, Training Time: 15.92 seconds\n",
            "Epoch 161/200, Loss: 0.017028056481055872, Training Time: 15.88 seconds\n",
            "Epoch 171/200, Loss: 0.014640708712256744, Training Time: 15.85 seconds\n",
            "Epoch 181/200, Loss: 0.012785245411774896, Training Time: 15.97 seconds\n",
            "Epoch 191/200, Loss: 0.011315543063030228, Training Time: 15.87 seconds\n",
            "Total Training Time: 3255.49 seconds\n",
            "Test Accuracy: 69.71%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Extend your CNN by adding one more additional convolution layer followed by an activation function and pooling function. You also need to adjust your fully connected layer properly with respect to intermediate feature dimensions. Train your network for 200 epochs."
      ],
      "metadata": {
        "id": "GPcKLMLzPRZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExtendedCIFAR10Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ExtendedCIFAR10Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 256)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.maxpool(x)\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.maxpool(x)\n",
        "        x = self.relu(self.conv3(x))\n",
        "        x = self.maxpool(x)\n",
        "        x = x.view(-1, 128 * 4 * 4)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Instantiate the model\n",
        "model = ExtendedCIFAR10Net().to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "epochs = 200\n",
        "total_start_time = time.time()\n",
        "for epoch in range(epochs):\n",
        "    start_time = time.time()\n",
        "    model.train()\n",
        "    losses = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses += loss.item()\n",
        "\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {losses/len(train_loader)}, Training Time: {training_time:.2f} seconds')\n",
        "\n",
        "\n",
        "total_end_time = time.time()\n",
        "total_training_time = total_end_time - total_start_time\n",
        "print(f'Total Training Time: {total_training_time:.2f} seconds')\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in val_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "val_accuracy = correct / total\n",
        "print(f'Test Accuracy: {val_accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iF3zfCYjPUbL",
        "outputId": "3f4f325b-cfba-4bf8-9d75-afa7ab46ff49"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200, Loss: 2.2927755951042443, Training Time: 17.25 seconds\n",
            "Epoch 11/200, Loss: 1.4788093929937538, Training Time: 17.81 seconds\n",
            "Epoch 21/200, Loss: 1.202768693462977, Training Time: 17.60 seconds\n",
            "Epoch 31/200, Loss: 1.0253927582590074, Training Time: 17.11 seconds\n",
            "Epoch 41/200, Loss: 0.8864922707308124, Training Time: 16.78 seconds\n",
            "Epoch 51/200, Loss: 0.7688095677913341, Training Time: 16.85 seconds\n",
            "Epoch 61/200, Loss: 0.6643369033103255, Training Time: 17.35 seconds\n",
            "Epoch 71/200, Loss: 0.5681727902578835, Training Time: 17.48 seconds\n",
            "Epoch 81/200, Loss: 0.4747468213755125, Training Time: 17.07 seconds\n",
            "Epoch 91/200, Loss: 0.38337138096396156, Training Time: 16.92 seconds\n",
            "Epoch 101/200, Loss: 0.2950749139296116, Training Time: 17.33 seconds\n",
            "Epoch 111/200, Loss: 0.2111034370834867, Training Time: 16.74 seconds\n",
            "Epoch 121/200, Loss: 0.13933392756774077, Training Time: 17.60 seconds\n",
            "Epoch 131/200, Loss: 0.08268350430645682, Training Time: 17.50 seconds\n",
            "Epoch 141/200, Loss: 0.03897636392560769, Training Time: 16.61 seconds\n",
            "Epoch 151/200, Loss: 0.019053895737152365, Training Time: 16.83 seconds\n",
            "Epoch 161/200, Loss: 0.011104351622681119, Training Time: 17.63 seconds\n",
            "Epoch 171/200, Loss: 0.007528456543338076, Training Time: 17.10 seconds\n",
            "Epoch 181/200, Loss: 0.005547009437447455, Training Time: 16.92 seconds\n",
            "Epoch 191/200, Loss: 0.0043346745763686075, Training Time: 16.85 seconds\n",
            "Total Training Time: 3433.13 seconds\n",
            "Test Accuracy: 72.33%\n"
          ]
        }
      ]
    }
  ]
}