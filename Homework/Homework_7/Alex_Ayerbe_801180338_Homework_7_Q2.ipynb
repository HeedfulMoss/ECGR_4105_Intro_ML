{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Build a ResNet based Convolutional Neural Network, like what we built in lectures (with skip connections), to classify the images across all 10 classes in CIFAR 10. For this problem, let’s use 10 blocks for ResNet and call it ResNet-10. Use the similar dimensions and channels as we need in lectures. Train your network for 200 epochs."
      ],
      "metadata": {
        "id": "2cRfJ3eQt-jZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yuMckU7SQXGo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ba42c1d-a502-497c-8632-a3de70d9cd34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data-unversions/p1ch7/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:23<00:00, 7.18MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data-unversions/p1ch7/cifar-10-python.tar.gz to ../data-unversions/p1ch7/\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import time\n",
        "\n",
        "datapath = '../data-unversions/p1ch7/'\n",
        "cifar10 = datasets.CIFAR10(root= datapath, train=True, download = True, transform=transforms.ToTensor())\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-processing. Seperating training and validation datasets."
      ],
      "metadata": {
        "id": "sSf9Qg4CQeJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imgs = torch.stack([img_t for img_t, _ in cifar10], dim=3)\n",
        "mean = imgs.view(3, -1).mean(dim=1)\n",
        "std = imgs.view(3, -1).std(dim=1)\n",
        "\n",
        "normalize = transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean,std)])\n",
        "\n",
        "train_data = datasets.CIFAR10(root= datapath, train=True, download = True, transform=normalize)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "\n",
        "valid_data = datasets.CIFAR10(root= datapath, train=False, download = True, transform=normalize)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "GNYm08NqQbtA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38fc0be9-d13f-49c7-f582-6e7669dcfdaf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch Normalization ResNet Training and Validation"
      ],
      "metadata": {
        "id": "Z-4AbJRFuWQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, inputs, outputs, stride=1):\n",
        "        super(Block, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inputs, outputs, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(outputs)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(outputs, outputs, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(outputs)\n",
        "        self.downsample = nn.Sequential()\n",
        "        if stride != 1 or inputs != outputs:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(inputs, outputs, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(outputs),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += self.downsample(identity)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet10(nn.Module):\n",
        "    def __init__(self, block, layers):\n",
        "        super(ResNet10, self).__init__()\n",
        "        self.inputs = 16\n",
        "        self.conv1 = nn.Conv2d(3, self.inputs, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(self.inputs)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.blocks = self._make_layers(block, layers)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(self.inputs, 10)\n",
        "\n",
        "    def _make_layers(self, block, layers):\n",
        "        layers_list = []\n",
        "        for layer in layers:\n",
        "            layers_list.append(block(self.inputs, self.inputs, stride=1))\n",
        "        return nn.Sequential(*layers_list)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.blocks(x)\n",
        "        x = self.avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model with Batch Normalization\n",
        "model = ResNet10(Block, [2, 2, 2]).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer= optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# Training loop\n",
        "epochs = 200\n",
        "total_start_time = time.time()\n",
        "for epoch in range(epochs):\n",
        "    start_time = time.time()\n",
        "    model.train()\n",
        "    losses = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses += loss.item()\n",
        "\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {losses/len(train_loader)}, Training Time: {training_time:.2f} seconds')\n",
        "\n",
        "total_end_time = time.time()\n",
        "total_training_time = total_end_time - total_start_time\n",
        "\n",
        "# Validation\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in valid_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "valid_accuracy = correct / total\n",
        "\n",
        "print(f'Batch Normalization Training Time: {total_training_time:.2f} seconds')\n",
        "print(f'Batch Normalization Final Training Loss: {losses / len(train_loader)}')\n",
        "print(f'Batch Normalization Final Validation Accuracy: {valid_accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Of5dJ2buXED",
        "outputId": "363fd945-76d7-456e-bdd9-0346c07e07a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200, Loss: 1.677608071385747, Training Time: 19.85 seconds\n",
            "Epoch 11/200, Loss: 0.8672370893113753, Training Time: 18.88 seconds\n",
            "Epoch 21/200, Loss: 0.7534167638901249, Training Time: 18.55 seconds\n",
            "Epoch 31/200, Loss: 0.6868878088872451, Training Time: 18.62 seconds\n",
            "Epoch 41/200, Loss: 0.647835611306188, Training Time: 18.30 seconds\n",
            "Epoch 51/200, Loss: 0.6142463703899432, Training Time: 18.03 seconds\n",
            "Epoch 61/200, Loss: 0.589586685342557, Training Time: 17.88 seconds\n",
            "Epoch 71/200, Loss: 0.5637577331584432, Training Time: 18.04 seconds\n",
            "Epoch 81/200, Loss: 0.5478988943426201, Training Time: 18.42 seconds\n",
            "Epoch 91/200, Loss: 0.5297096873945593, Training Time: 18.35 seconds\n",
            "Epoch 101/200, Loss: 0.5165627595523129, Training Time: 18.52 seconds\n",
            "Epoch 111/200, Loss: 0.5006466954374862, Training Time: 18.26 seconds\n",
            "Epoch 121/200, Loss: 0.48845816460792973, Training Time: 17.83 seconds\n",
            "Epoch 131/200, Loss: 0.4741320013999939, Training Time: 17.76 seconds\n",
            "Epoch 141/200, Loss: 0.4715070529552677, Training Time: 18.14 seconds\n",
            "Epoch 151/200, Loss: 0.4587618509483764, Training Time: 17.73 seconds\n",
            "Epoch 161/200, Loss: 0.44852776004149175, Training Time: 18.03 seconds\n"
          ]
        }
      ]
    }
  ]
}